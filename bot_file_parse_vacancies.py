import os
import re
import requests
import logging
from time import sleep
from telegram import Update
from telegram.ext import Updater, CommandHandler, MessageHandler, Filters, CallbackContext
from bs4 import BeautifulSoup
import pandas as pd

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –¢–æ–∫–µ–Ω –±–æ—Ç–∞
TOKEN = os.environ.get("BOT_TOKEN")
if not TOKEN:
    raise ValueError("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è BOT_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")

# –ö–∞—Ä—Ç–∞ –≥–æ—Ä–æ–¥–æ–≤ –∏ –∏—Ö ID –Ω–∞ hh.ru
AREA_MAP = {
    "–º–æ—Å–∫–≤–∞": 1,
    "—Å–∞–Ω–∫—Ç-–ø–µ—Ç–µ—Ä–±—É—Ä–≥": 2,
    "–Ω–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫": 4,
    "–µ–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥": 3,
    "–Ω–∏–∂–Ω–∏–π –Ω–æ–≤–≥–æ—Ä–æ–¥": 66,
    "–∫–∞–∑–∞–Ω—å": 88,
    "—á–µ–ª—è–±–∏–Ω—Å–∫": 104,
    "–æ–º—Å–∫": 68,
    "—Å–∞–º–∞—Ä–∞": 78,
    "—Ä–æ—Å—Ç–æ–≤-–Ω–∞-–¥–æ–Ω—É": 76,
    "—É—Ñ–∞": 99,
    "–∫—Ä–∞—Å–Ω–æ—è—Ä—Å–∫": 54,
    "–ø–µ—Ä–º—å": 72,
    "–≤–æ—Ä–æ–Ω–µ–∂": 106,
    "–≤–æ–ª–≥–æ–≥—Ä–∞–¥": 102,
    "–∫—Ä–∞—Å–Ω–æ–¥–∞—Ä": 53  # ‚Üê –¥–æ–±–∞–≤–ª–µ–Ω –ö—Ä–∞—Å–Ω–æ–¥–∞—Ä
}

SCHEDULE_MAP = {
    "–ø–æ–ª–Ω—ã–π –¥–µ–Ω—å": "fullDay",
    "—Å–º–µ–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫": "shift",
    "–≥–∏–±–∫–∏–π –≥—Ä–∞—Ñ–∏–∫": "flexible",
    "—É–¥–∞–ª—ë–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞": "remote",
    "–≤–∞—Ö—Ç–æ–≤—ã–π –º–µ—Ç–æ–¥": "flyInFlyOut"
}

def extract_area(text):
    for city, area_id in AREA_MAP.items():
        if city in text.lower():
            return area_id, city
    return 113, None  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –≤—Å—è –†–æ—Å—Å–∏—è

def clean_html(html_text):
    return BeautifulSoup(html_text, "html.parser").get_text()

def get_full_description(vacancy_id):
    try:
        url = f"https://api.hh.ru/vacancies/{vacancy_id}"
        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        data = response.json()
        return data.get("description", "–ù–µ—Ç –ø–æ–ª–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è")
    except Exception as e:
        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è id {vacancy_id}: {e}")
        return "–ù–µ—Ç –ø–æ–ª–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è"

def extract_filters(text):
    salary_match = re.search(r'–∑–∞—Ä–ø–ª–∞—Ç–∞\s*>\s*(\d+)', text, re.IGNORECASE)
    employment_match = re.search(r'—Ç–∏–ø\s+–∑–∞–Ω—è—Ç–æ—Å—Ç–∏\s*:\s*(\w+)', text, re.IGNORECASE)
    schedule_match = re.search(r'–≥—Ä–∞—Ñ–∏–∫\s+—Ä–∞–±–æ—Ç—ã\s*:\s*([\w\s\-]+)', text, re.IGNORECASE)

    filters = {
        "salary": int(salary_match.group(1)) if salary_match else None,
        "employment": employment_match.group(1) if employment_match else None
        "schedule": schedule_match.group(1).strip().lower() if schedule_match else None
    }
    return filters

def get_vacancies(search_text: str, page: int = 0, per_page: int = 50, salary_from=None, employment=None, schedule=None, area=1):
    """–ü–æ–ª—É—á–∞–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ —Å hh.ru"""
    try:
        url = "https://api.hh.ru/vacancies"
        params = {
            "text": search_text,
            "page": page,
            "per_page": per_page,
            "area": area,  # 1 - –ú–æ—Å–∫–≤–∞ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        }
        if salary_from:
            params["salary"] = salary_from
        if employment:
            params["employment"] = employment
        if schedule:
            schedule_api = SCHEDULE_MAP.get(schedule.lower())
            if schedule_api:
                params["schedule"] = schedule_api

        headers = {'User-Agent': 'Mozilla/5.0'}
        response = requests.get(url, params=params, headers=headers, timeout=10)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ API: {e}")
        return None

def parse_vacancies(data: dict):
    """–ü–∞—Ä—Å–∏—Ç –¥–∞–Ω–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–π"""
    if not data or 'items' not in data:
        return []
    
    vacancies = []
    for item in data.get("items", []):
        try:
            salary = item.get("salary")
            if salary is None:
                salary_info = "–ù–µ —É–∫–∞–∑–∞–Ω–∞"
            else:
                salary_from = salary.get('from', '?')
                salary_to = salary.get('to', '?')
                currency = salary.get('currency', '')
                salary_info = f"{salary_from} - {salary_to} {currency}".strip()
            
            vacancies.append({
                "name": item.get("name", "–ù–∞–∑–≤–∞–Ω–∏–µ –Ω–µ —É–∫–∞–∑–∞–Ω–æ"),
                "company": item.get("employer", {}).get("name", "–ö–æ–º–ø–∞–Ω–∏—è –Ω–µ —É–∫–∞–∑–∞–Ω–∞"),
                "salary": salary_info,
                "description": clean_html(get_full_description(item["id"])),
                "url": item.get("alternate_url", "#"),
            })
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–∏: {e}")
    return vacancies

def save_to_xlsx(vacancies: list, filename: str = "vacancies.xlsx"):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤–∞–∫–∞–Ω—Å–∏–∏ –≤ XLSX"""
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        filepath = os.path.join(script_dir, filename)
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        df = pd.DataFrame(vacancies, columns=['name', 'company', 'salary', 'description', 'url'])
        df.columns = ['–î–æ–ª–∂–Ω–æ—Å—Ç—å', '–ö–æ–º–ø–∞–Ω–∏—è', '–ó–∞—Ä–ø–ª–∞—Ç–∞', '–û–ø–∏—Å–∞–Ω–∏–µ', '–°—Å—ã–ª–∫–∞']
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ XLSX
        df.to_excel(filepath, index=False, engine='openpyxl')
        
        return filepath
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è XLSX: {e}")
        return None

def start(update: Update, context: CallbackContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∫–æ–º–∞–Ω–¥—ã /start"""
    try:
        update.message.reply_text(
            "–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤–∞–∫–∞–Ω—Å–∏–π —Å hh.ru\n"
            "–û—Ç–ø—Ä–∞–≤—å –º–Ω–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä: Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫)\n\n"
            "–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å:\n"
            "- –∑–∞—Ä–ø–ª–∞—Ç–∞ > 100000\n"
            "- —Ç–∏–ø –∑–∞–Ω—è—Ç–æ—Å—Ç–∏: full\n"
            "–ü—Ä–∏–º–µ—Ä: Python –∑–∞—Ä–ø–ª–∞—Ç–∞ > 150000 —Ç–∏–ø –∑–∞–Ω—è—Ç–æ—Å—Ç–∏: part"
        )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≤ –∫–æ–º–∞–Ω–¥–µ /start: {e}")

def handle_message(update: Update, context: CallbackContext):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
    try:
        search_query = update.message.text
        filters = extract_filters(search_query)

        area_id, area_name = extract_area(search_query)
        clean_text = search_query 

        if area_name:
            clean_text = re.sub(r'–≤\s+' + re.escape(area_name), '', clean_text, flags=re.IGNORECASE)

        clean_text = re.sub(r'–∑–∞—Ä–ø–ª–∞—Ç–∞\s*>\s*\d+', '', clean_text, flags=re.IGNORECASE)
        clean_text = re.sub(r'—Ç–∏–ø\s+–∑–∞–Ω—è—Ç–æ—Å—Ç–∏\s*:\s*\w+', '', clean_text, flags=re.IGNORECASE).strip()


        logger.info(f"–ü–æ–ª—É—á–µ–Ω –∑–∞–ø—Ä–æ—Å: {search_query}")
        update.message.reply_text(f"üîç –ò—â—É –≤–∞–∫–∞–Ω—Å–∏–∏ –ø–æ –∑–∞–ø—Ä–æ—Å—É: {search_query}...")
        
        all_vacancies = []
        for page in range(3):  # –ü–∞—Ä—Å–∏–º 3 —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            try:
                data = get_vacancies(
                    clean_text,
                    page=page,
                    salary_from=filters["salary"],
                    employment=filters["employment"],
                    schedule=filters["schedule"],
                    area=area_id
                )
                if data:
                    all_vacancies.extend(parse_vacancies(data))
                sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
        
        if not all_vacancies:
            update.message.reply_text("üòï –í–∞–∫–∞–Ω—Å–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return
            
        xlsx_path = save_to_xlsx(all_vacancies)
        if xlsx_path:
            try:
                with open(xlsx_path, 'rb') as file:
                    update.message.reply_document(
                        document=file,
                        caption=f"–ù–∞–π–¥–µ–Ω–æ {len(all_vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π\n–§–∞–π–ª –≤ —Ñ–æ—Ä–º–∞—Ç–µ XLSX",
                        filename="vacancies.xlsx"
                    )
                
                preview = "\n".join(
                    f"{i+1}. {v['name']} ({v['company']}) - {v['salary']}\n{v['url']}"
                    for i, v in enumerate(all_vacancies[:5])
                )
                update.message.reply_text(f"üìã –ü—Ä–∏–º–µ—Ä—ã –≤–∞–∫–∞–Ω—Å–∏–π:\n\n{preview}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Ñ–∞–π–ª–∞: {e}")
                update.message.reply_text("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
        update.message.reply_text("‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ –±–æ—Ç–∞")

def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    try:
        logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
        updater = Updater(TOKEN, use_context=True)
        dp = updater.dispatcher
        
        dp.add_handler(CommandHandler("start", start))
        dp.add_handler(CommandHandler("help", start))
        dp.add_handler(MessageHandler(Filters.text & ~Filters.command, handle_message))
        
        logger.info("–ë–æ—Ç —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω –∏ –æ–∂–∏–¥–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–π...")
        updater.start_polling()
        updater.idle()
    except Exception as e:
        logger.critical(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞: {e}")

if __name__ == "__main__":
    main()